{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d9979d-bbeb-400e-a494-3eb067c2a391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ POWER UNLEASHED: Using Apple M4 (MPS) Acceleration\n",
      "\n",
      "üîç Scanning for Test Data...\n",
      "   ‚úÖ Track A File: test_track_a.jsonl\n",
      "   ‚úÖ Track B File: test_track_b.jsonl\n",
      "\n",
      "‚¨áÔ∏è STARTING DOWNLOADS (This ensures you see progress)...\n",
      "   1. Downloading DeBERTa-v3-Large (~800MB)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 17 files:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 5/17 [26:16<1:03:03, 315.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                   initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:608\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:440\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   1. Downloading DeBERTa-v3-Large (~800MB)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m model_a_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross-encoder/nli-deberta-v3-large\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 43\u001b[0m \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_a_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# This triggers the bar\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Model 2: GTE Large\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   2. Downloading GTE-Large-v1.5 (~1.5GB)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py:332\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[1;32m    330\u001b[0m         _inner_hf_hub_download(file)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(local_dir))\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tqdm_class(ex\u001b[38;5;241m.\u001b[39mmap(fn, \u001b[38;5;241m*\u001b[39miterables, chunksize\u001b[38;5;241m=\u001b[39mchunksize), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:636\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 636\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py:229\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 229\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:1053\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:1069\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1070\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from huggingface_hub import snapshot_download # This helps us show download bars\n",
    "import torch\n",
    "\n",
    "# --- 1. HARDWARE SETUP ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ POWER UNLEASHED: Using Apple M4 (MPS) Acceleration\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"‚ö†Ô∏è WARNING: Running on CPU.\")\n",
    "\n",
    "# --- 2. FIND FILES ---\n",
    "print(\"\\nüîç Scanning for Test Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "\n",
    "if not input_a or not input_b:\n",
    "    input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "    print(\"‚ö†Ô∏è Using manual filenames (Auto-detection failed)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Track A File: {input_a}\")\n",
    "    print(f\"   ‚úÖ Track B File: {input_b}\")\n",
    "\n",
    "# --- 3. DOWNLOAD MODELS WITH PROGRESS BARS ---\n",
    "print(\"\\n‚¨áÔ∏è STARTING DOWNLOADS (This ensures you see progress)...\")\n",
    "\n",
    "# Model 1: DeBERTa v3 Large\n",
    "print(\"   1. Downloading DeBERTa-v3-Large (~800MB)...\")\n",
    "model_a_id = 'cross-encoder/nli-deberta-v3-large'\n",
    "snapshot_download(repo_id=model_a_id) # This triggers the bar\n",
    "\n",
    "# Model 2: GTE Large\n",
    "print(\"   2. Downloading GTE-Large-v1.5 (~1.5GB)...\")\n",
    "model_b_id = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "snapshot_download(repo_id=model_b_id) # This triggers the bar\n",
    "\n",
    "print(\"\\n‚úÖ Downloads Complete. Loading into Memory...\")\n",
    "model_a = CrossEncoder(model_a_id, device=device)\n",
    "model_b = SentenceTransformer(model_b_id, trust_remote_code=True, device=device)\n",
    "\n",
    "# --- 4. EXECUTE TRACK A ---\n",
    "print(f\"\\nüß† SCORING TRACK A ({input_a})...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "\n",
    "# Column detection\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "# Create pairs\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# INFERENCE (Progress bar included)\n",
    "scores_a = model_a.predict(pairs_a, batch_size=4, show_progress_bar=True)\n",
    "scores_b = model_a.predict(pairs_b, batch_size=4, show_progress_bar=True)\n",
    "preds_a = scores_a > scores_b\n",
    "\n",
    "# --- 5. EXECUTE TRACK B ---\n",
    "print(f\"\\nüß† EMBEDDING TRACK B ({input_b})...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    # INFERENCE (Progress bar included)\n",
    "    embeddings = model_b.encode(\n",
    "        df_b[text_col].tolist(), \n",
    "        batch_size=4, \n",
    "        show_progress_bar=True, \n",
    "        device=device,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. ZIP IT UP ---\n",
    "print(\"\\nüì¶ Zipping Final Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_SOTA_FINAL.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ READY! Upload '{zip_name}' to CodaBench Testing Phase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe901261-daf4-4839-9477-8eb75c1a5b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple M4 (MPS) - Ready for SOTA Models\n",
      "üîç Scanning for Test Data...\n",
      "   Track A File: test_track_a.jsonl\n",
      "   Track B File: test_track_b.jsonl\n",
      "\n",
      "üìÇ Loading Models from your local download...\n",
      "   ‚úÖ Found local DeBERTa model! Loading...\n",
      "   ‚úÖ Found local GTE model! Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Scoring Track A (DeBERTa)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:17<00:00,  3.94s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:59<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Embedding Track B (GTE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:24<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Zipping Submission...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/track_a.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m preds_a:\n\u001b[0;32m---> 97\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_a_is_closer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m}, f)\n\u001b[1;32m     98\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/track_b.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# --- 1. HARDWARE SETUP ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - Ready for SOTA Models\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"‚ö†Ô∏è Using CPU (Slow)\")\n",
    "\n",
    "# --- 2. FIND TEST FILES ---\n",
    "print(\"üîç Scanning for Test Data...\")\n",
    "# We look for files with the correct line counts (400 and 849)\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "\n",
    "# Fallback if auto-detection fails\n",
    "if not input_a or not input_b: \n",
    "    input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "\n",
    "print(f\"   Track A File: {input_a}\\n   Track B File: {input_b}\")\n",
    "\n",
    "# --- 3. LOAD MODELS FROM LOCAL FOLDERS ---\n",
    "print(\"\\nüìÇ Loading Models from your local download...\")\n",
    "\n",
    "# These match the folder names from the terminal command\n",
    "path_a = './nli-deberta-v3-large'\n",
    "path_b = './gte-large-en-v1.5'\n",
    "\n",
    "# Load Model A (DeBERTa)\n",
    "if os.path.exists(path_a):\n",
    "    print(\"   ‚úÖ Found local DeBERTa model! Loading...\")\n",
    "    model_a = CrossEncoder(path_a, device=device)\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: Could not find folder '{path_a}'. Check where you ran the terminal command.\")\n",
    "\n",
    "# Load Model B (GTE)\n",
    "if os.path.exists(path_b):\n",
    "    print(\"   ‚úÖ Found local GTE model! Loading...\")\n",
    "    model_b = SentenceTransformer(path_b, trust_remote_code=True, device=device)\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: Could not find folder '{path_b}'.\")\n",
    "\n",
    "# --- 4. RUN TRACK A (SCORING) ---\n",
    "print(f\"\\nüß† Scoring Track A (DeBERTa)...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "\n",
    "# Column detection\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# Batch size 8 is safe for M4 with these local models\n",
    "scores_a = model_a.predict(pairs_a, batch_size=8, show_progress_bar=True)\n",
    "scores_b = model_a.predict(pairs_b, batch_size=8, show_progress_bar=True)\n",
    "preds_a = scores_a > scores_b\n",
    "\n",
    "# --- 5. RUN TRACK B (EMBEDDING) ---\n",
    "print(f\"\\nüß† Embedding Track B (GTE)...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    embeddings = model_b.encode(\n",
    "        df_b[text_col].tolist(), \n",
    "        batch_size=4, # GTE Large is big, keep batch small\n",
    "        show_progress_bar=True, \n",
    "        device=device,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. ZIP AND FINISH ---\n",
    "print(\"\\nüì¶ Zipping Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_SOTA_LOCAL.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ READY! Upload '{zip_name}' to CodaBench Testing Phase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb570b74-db6e-4e69-a903-bf5c90fa93ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Applying Fix for NLI Model Output...\n",
      "   - Detected multi-column scores (400, 3). Extracting 'Entailment' (Index 1)...\n",
      "   - Re-calculated 400 predictions.\n",
      "üì¶ Zipping Final Submission...\n",
      "\n",
      "üèÜ SUCCESS! Upload 'submission_SOTA_FIXED.zip' to CodaBench Testing Phase.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"üõ†Ô∏è Applying Fix for NLI Model Output...\")\n",
    "\n",
    "# 1. FIX THE SCORES\n",
    "# The model output shape is likely (400, 3). We want column 1 (Entailment).\n",
    "# We check the shape to be safe.\n",
    "if len(scores_a.shape) > 1 and scores_a.shape[1] >= 2:\n",
    "    print(f\"   - Detected multi-column scores {scores_a.shape}. Extracting 'Entailment' (Index 1)...\")\n",
    "    final_scores_a = scores_a[:, 1]\n",
    "    final_scores_b = scores_b[:, 1]\n",
    "else:\n",
    "    # Fallback if it was already 1D\n",
    "    final_scores_a = scores_a\n",
    "    final_scores_b = scores_b\n",
    "\n",
    "# 2. RE-CALCULATE PREDICTIONS\n",
    "# Now we compare single numbers, so we get a clean True/False list\n",
    "preds_a = final_scores_a > final_scores_b\n",
    "print(f\"   - Re-calculated {len(preds_a)} predictions.\")\n",
    "\n",
    "# 3. SAVE & ZIP (Standard Routine)\n",
    "print(\"üì¶ Zipping Final Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        # This will now work because 'val' is a simple Python boolean\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "# We re-save Track B just to be sure (it was already fine, but good to keep in sync)\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_SOTA_FIXED.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ SUCCESS! Upload '{zip_name}' to CodaBench Testing Phase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d551f3dd-ab10-4535-98bd-645c9407807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç FINAL VERIFICATION PROTOCOL INITIATED...\n",
      "\n",
      "Checking Track A (outputs/track_a.jsonl)...\n",
      "   ‚úÖ Key 'text_a_is_closer' found.\n",
      "   ‚úÖ Value type is BOOLEAN (False).\n",
      "   ‚úÖ Line count is exactly 400.\n",
      "\n",
      "Checking Track B (outputs/track_b.jsonl)...\n",
      "   ‚úÖ Key 'embedding' (singular) found.\n",
      "   ‚úÖ Value is a LIST of floats (Length: 1024).\n",
      "   ‚úÖ Line count is exactly 849.\n",
      "\n",
      "Checking Zip Archive (submission_SOTA_FIXED.zip)...\n",
      "   ‚úÖ Zip contains correct files: ['track_a.jsonl', 'track_b.jsonl']\n",
      "\n",
      "üöÄ VERIFICATION COMPLETE. If all ticks are Green, you are safe to upload.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"üîç FINAL VERIFICATION PROTOCOL INITIATED...\\n\")\n",
    "\n",
    "# 1. Verify Track A\n",
    "print(\"Checking Track A (outputs/track_a.jsonl)...\")\n",
    "try:\n",
    "    with open('outputs/track_a.jsonl', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        count_a = len(lines)\n",
    "        first_line = json.loads(lines[0])\n",
    "        \n",
    "        # Check 1: Key Name\n",
    "        if \"text_a_is_closer\" in first_line:\n",
    "            print(f\"   ‚úÖ Key 'text_a_is_closer' found.\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå CRITICAL: Wrong key in Track A. Found: {first_line.keys()}\")\n",
    "            \n",
    "        # Check 2: Value Type\n",
    "        val = first_line[\"text_a_is_closer\"]\n",
    "        if isinstance(val, bool):\n",
    "            print(f\"   ‚úÖ Value type is BOOLEAN ({val}).\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå CRITICAL: Wrong type. Expected bool, got {type(val)}.\")\n",
    "            \n",
    "        # Check 3: Count\n",
    "        if count_a == 400:\n",
    "            print(f\"   ‚úÖ Line count is exactly 400.\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è WARNING: Line count is {count_a} (Expected 400).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error reading Track A: {e}\")\n",
    "\n",
    "# 2. Verify Track B\n",
    "print(\"\\nChecking Track B (outputs/track_b.jsonl)...\")\n",
    "try:\n",
    "    with open('outputs/track_b.jsonl', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        count_b = len(lines)\n",
    "        first_line = json.loads(lines[0])\n",
    "        \n",
    "        # Check 1: Key Name\n",
    "        if \"embedding\" in first_line:\n",
    "            print(f\"   ‚úÖ Key 'embedding' (singular) found.\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå CRITICAL: Wrong key. Found: {first_line.keys()}\")\n",
    "            \n",
    "        # Check 2: Value Type & Shape\n",
    "        emb = first_line[\"embedding\"]\n",
    "        if isinstance(emb, list) and len(emb) > 10:\n",
    "            print(f\"   ‚úÖ Value is a LIST of floats (Length: {len(emb)}).\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå CRITICAL: Invalid embedding format.\")\n",
    "            \n",
    "        # Check 3: Count\n",
    "        if count_b == 849:\n",
    "            print(f\"   ‚úÖ Line count is exactly 849.\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è WARNING: Line count is {count_b} (Expected ~849).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error reading Track B: {e}\")\n",
    "\n",
    "# 3. Verify Zip File\n",
    "print(\"\\nChecking Zip Archive (submission_SOTA_FIXED.zip)...\")\n",
    "try:\n",
    "    with zipfile.ZipFile('submission_BGE.zip', 'r') as z:\n",
    "        files = z.namelist()\n",
    "        if 'track_a.jsonl' in files and 'track_b.jsonl' in files:\n",
    "            print(f\"   ‚úÖ Zip contains correct files: {files}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå CRITICAL: Zip is missing files. Found: {files}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error checking Zip: {e}\")\n",
    "\n",
    "print(\"\\nüöÄ VERIFICATION COMPLETE. If all ticks are Green, you are safe to upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9decb094-f4cd-449f-8113-ee7ca470c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple M4 (MPS) - BGE Edition\n",
      "üîç Finding Test Data...\n",
      "   Track A: test_track_a.jsonl\n",
      "   Track B: test_track_b.jsonl\n",
      "\n",
      "üìÇ Loading BGE Models...\n",
      "   ‚úÖ Loading Reranker (Track A)...\n",
      "   ‚úÖ Loading Embedder (Track B)...\n",
      "\n",
      "üß† Scoring Track A (BGE Reranker)...\n",
      "   - Calculating scores...\n",
      "\n",
      "üß† Embedding Track B (BGE Large)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [01:04<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Zipping BGE Submission...\n",
      "\n",
      "üèÜ READY! Upload 'submission_BGE.zip' to CodaBench.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# --- 1. HARDWARE ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - BGE Edition\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# --- 2. FIND TEST FILES ---\n",
    "print(\"üîç Finding Test Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "if not input_a or not input_b: input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "print(f\"   Track A: {input_a}\\n   Track B: {input_b}\")\n",
    "\n",
    "# --- 3. LOAD LOCAL BGE MODELS ---\n",
    "print(\"\\nüìÇ Loading BGE Models...\")\n",
    "\n",
    "# Path A: Reranker\n",
    "path_a = './bge-reranker-large'\n",
    "if os.path.exists(path_a):\n",
    "    print(\"   ‚úÖ Loading Reranker (Track A)...\")\n",
    "    # Rerankers are loaded slightly differently than CrossEncoders\n",
    "    tokenizer_a = AutoTokenizer.from_pretrained(path_a)\n",
    "    model_a = AutoModelForSequenceClassification.from_pretrained(path_a).to(device)\n",
    "    model_a.eval()\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: '{path_a}' not found. Did you run the terminal command?\")\n",
    "\n",
    "# Path B: Embedding\n",
    "path_b = './bge-large-en-v1.5'\n",
    "if os.path.exists(path_b):\n",
    "    print(\"   ‚úÖ Loading Embedder (Track B)...\")\n",
    "    model_b = SentenceTransformer(path_b, device=device)\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: '{path_b}' not found.\")\n",
    "\n",
    "# --- 4. RUN TRACK A (RERANKING) ---\n",
    "print(f\"\\nüß† Scoring Track A (BGE Reranker)...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "# Reranker expects simple pairs: [Anchor, A] and [Anchor, B]\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# Helper function for Reranker Inference\n",
    "def predict_reranker(pairs, model, tokenizer, batch_size=8):\n",
    "    scores = []\n",
    "    # Process in chunks\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[i:i+batch_size]\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            # Get logits (score)\n",
    "            output = model(**inputs).logits.view(-1).float()\n",
    "            scores.extend(output.cpu().numpy())\n",
    "    return scores\n",
    "\n",
    "print(\"   - Calculating scores...\")\n",
    "scores_a = predict_reranker(pairs_a, model_a, tokenizer_a)\n",
    "scores_b = predict_reranker(pairs_b, model_a, tokenizer_a)\n",
    "\n",
    "# Logic: Higher score = Better match\n",
    "preds_a = [s_a > s_b for s_a, s_b in zip(scores_a, scores_b)]\n",
    "\n",
    "# --- 5. RUN TRACK B (EMBEDDING) ---\n",
    "print(f\"\\nüß† Embedding Track B (BGE Large)...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    # BGE works best with a prompt for asymmetric tasks, but for symmetric story similarity\n",
    "    # we usually keep it raw. However, adding \"Represent this story:\" can sometimes help.\n",
    "    # Let's stick to raw for safety unless specified.\n",
    "    embeddings = model_b.encode(\n",
    "        df_b[text_col].tolist(), \n",
    "        batch_size=8, \n",
    "        show_progress_bar=True, \n",
    "        device=device,\n",
    "        normalize_embeddings=True # BGE requires normalized embeddings\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. SAVE & ZIP ---\n",
    "print(\"\\nüì¶ Zipping BGE Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_BGE.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ READY! Upload '{zip_name}' to CodaBench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5fe4e15-a7a2-4578-8643-f9de62366f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VERIFYING: submission_BGE.zip ...\n",
      "\n",
      "   ‚úÖ ZIP Structure: OK (Found both jsonl files)\n",
      "   ‚úÖ Track A Count: OK (400 items)\n",
      "   ‚úÖ Track A Format: OK (Key 'text_a_is_closer' is Boolean)\n",
      "   ‚úÖ Track B Count: OK (849 items)\n",
      "   ‚úÖ Track B Key: OK (Found singular 'embedding')\n",
      "   ‚úÖ Track B Dimensions: OK (1024 for BGE-Large)\n",
      "\n",
      "üöÄ STATUS: READY TO UPLOAD.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_filename = 'submission_BGE.zip'\n",
    "\n",
    "print(f\"üîç VERIFYING: {zip_filename} ...\\n\")\n",
    "\n",
    "if not os.path.exists(zip_filename):\n",
    "    print(f\"‚ùå CRITICAL ERROR: File '{zip_filename}' not found!\")\n",
    "else:\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as z:\n",
    "            files = z.namelist()\n",
    "            \n",
    "            # --- CHECK 1: FILE STRUCTURE ---\n",
    "            if 'track_a.jsonl' in files and 'track_b.jsonl' in files:\n",
    "                print(f\"   ‚úÖ ZIP Structure: OK (Found both jsonl files)\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå ZIP ERROR: Missing files. Found: {files}\")\n",
    "\n",
    "            # --- CHECK 2: TRACK A CONTENT ---\n",
    "            with z.open('track_a.jsonl') as f:\n",
    "                lines = f.readlines()\n",
    "                count = len(lines)\n",
    "                first = json.loads(lines[0])\n",
    "                \n",
    "                # Check Count\n",
    "                if count == 400:\n",
    "                    print(f\"   ‚úÖ Track A Count: OK (400 items)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è Track A Count: WARNING ({count} items - Expected 400)\")\n",
    "\n",
    "                # Check Key & Type\n",
    "                if \"text_a_is_closer\" in first and isinstance(first[\"text_a_is_closer\"], bool):\n",
    "                    print(f\"   ‚úÖ Track A Format: OK (Key 'text_a_is_closer' is Boolean)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Track A ERROR: Invalid JSON format: {first}\")\n",
    "\n",
    "            # --- CHECK 3: TRACK B CONTENT ---\n",
    "            with z.open('track_b.jsonl') as f:\n",
    "                lines = f.readlines()\n",
    "                count = len(lines)\n",
    "                first = json.loads(lines[0])\n",
    "                \n",
    "                # Check Count\n",
    "                if count == 849:\n",
    "                    print(f\"   ‚úÖ Track B Count: OK (849 items)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è Track B Count: WARNING ({count} items - Expected 849)\")\n",
    "\n",
    "                # Check Key (The most common error)\n",
    "                if \"embedding\" in first:\n",
    "                    print(f\"   ‚úÖ Track B Key: OK (Found singular 'embedding')\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Track B ERROR: Key mismatch! Found: {list(first.keys())} (Expected 'embedding')\")\n",
    "\n",
    "                # Check Vector Size (BGE-Large should be 1024)\n",
    "                vec_len = len(first[\"embedding\"])\n",
    "                if vec_len == 1024:\n",
    "                    print(f\"   ‚úÖ Track B Dimensions: OK (1024 for BGE-Large)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ÑπÔ∏è Track B Dimensions: {vec_len} (Just FYI)\")\n",
    "\n",
    "        print(\"\\nüöÄ STATUS: READY TO UPLOAD.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå SCRIPT CRASHED: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5760100e-3bc1-4602-b67c-793a5de0c4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krish/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/krish/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple M4 (MPS) - Optimized for STS\n",
      "üîç Scanning for Test Data...\n",
      "   ‚úÖ Track A: test_track_a.jsonl\n",
      "   ‚úÖ Track B: test_track_b.jsonl\n",
      "\n",
      "üìÇ Loading Models...\n",
      "   ‚úÖ Loading STS RoBERTa (Track A)...\n",
      "   ‚úÖ Loading Mxbai Large (Track B)...\n",
      "\n",
      "üß† Scoring Track A...\n",
      "   - Calculating similarity scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [01:15<00:00,  3.01s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [01:17<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Embedding Track B...\n",
      "   - Applying 'Represent this story' prompt for Mxbai...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [01:01<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Zipping Submission...\n",
      "\n",
      "üèÜ READY! Upload 'submission_STS_MXBAI.zip' to CodaBench.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "\n",
    "# --- 1. HARDWARE CHECK ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - Optimized for STS\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# --- 2. INTELLIGENT FILE FINDER ---\n",
    "print(\"üîç Scanning for Test Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "\n",
    "if not input_a or not input_b: \n",
    "    input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "    print(\"‚ö†Ô∏è Using manual filenames (Auto-detection failed)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Track A: {input_a}\")\n",
    "    print(f\"   ‚úÖ Track B: {input_b}\")\n",
    "\n",
    "# --- 3. LOAD THE \"SIMILARITY\" EXPERTS ---\n",
    "print(\"\\nüìÇ Loading Models...\")\n",
    "\n",
    "# Model A: STS RoBERTa (The Similarity Judge)\n",
    "path_a = './stsb-roberta-large'\n",
    "if os.path.exists(path_a):\n",
    "    print(\"   ‚úÖ Loading STS RoBERTa (Track A)...\")\n",
    "    model_a = CrossEncoder(path_a, device=device)\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: '{path_a}' not found. Did you run the terminal command?\")\n",
    "    # Fallback just in case\n",
    "    model_a = CrossEncoder('cross-encoder/stsb-roberta-large', device=device)\n",
    "\n",
    "# Model B: Mxbai Large (The Context Mapper)\n",
    "path_b = './mxbai-embed-large-v1'\n",
    "if os.path.exists(path_b):\n",
    "    print(\"   ‚úÖ Loading Mxbai Large (Track B)...\")\n",
    "    model_b = SentenceTransformer(path_b, device=device)\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: '{path_b}' not found.\")\n",
    "    model_b = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1', device=device)\n",
    "\n",
    "# --- 4. EXECUTE TRACK A (Pairwise Similarity) ---\n",
    "print(f\"\\nüß† Scoring Track A...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "\n",
    "# Column Setup\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# Predict: This model outputs a single float (0 to 1) score representing similarity\n",
    "print(\"   - Calculating similarity scores...\")\n",
    "scores_a = model_a.predict(pairs_a, batch_size=16, show_progress_bar=True)\n",
    "scores_b = model_a.predict(pairs_b, batch_size=16, show_progress_bar=True)\n",
    "\n",
    "# If Score A > Score B, then A is the winner\n",
    "preds_a = scores_a > scores_b\n",
    "\n",
    "# --- 5. EXECUTE TRACK B (Prompt-Based Embedding) ---\n",
    "print(f\"\\nüß† Embedding Track B...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    print(\"   - Applying 'Represent this story' prompt for Mxbai...\")\n",
    "    # Mxbai works best with a specific prompt instruction\n",
    "    prompt = \"Represent this story for semantic similarity search: \"\n",
    "    texts = [prompt + t for t in df_b[text_col].tolist()]\n",
    "    \n",
    "    embeddings = model_b.encode(\n",
    "        texts, \n",
    "        batch_size=8, \n",
    "        show_progress_bar=True, \n",
    "        device=device,\n",
    "        normalize_embeddings=True # Crucial for Mxbai\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. SAVE & ZIP ---\n",
    "print(\"\\nüì¶ Zipping Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f) # Singular 'embedding'\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_STS_MXBAI.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ READY! Upload '{zip_name}' to CodaBench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c9b2609-5d89-4fd3-a05f-6abe2e6dc111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple M4 (MPS) - Running Ensemble Mode\n",
      "üîç Scanning Test Data...\n",
      "   Track A: test_track_a.jsonl\n",
      "   Track B: test_track_b.jsonl\n",
      "\n",
      "üìÇ Summoning the Models...\n",
      "   ‚úÖ Loaded Expert 1: Logic (DeBERTa)\n",
      "   ‚úÖ Loaded Expert 2: Vibe (RoBERTa)\n",
      "   ‚úÖ Loaded Expert 3: Narrative (Jina v3)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/krish/.cache/huggingface/modules/transformers_modules/jinaai/xlm_hyphen_roberta_hyphen_flash_hyphen_implementation/2b6bc3f30750b3a9648fe9b63448c09920efe9be/mha.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ‚úÖ Loaded Expert 3: Narrative (Jina v3)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Jina requires trust_remote_code for its special architecture\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model_jina \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_jina\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ‚ö†Ô∏è Local Jina not found. Downloading...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:327\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    309\u001b[0m has_modules \u001b[38;5;241m=\u001b[39m is_sentence_transformer_model(\n\u001b[1;32m    310\u001b[0m     model_name_or_path,\n\u001b[1;32m    311\u001b[0m     token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    317\u001b[0m     has_modules\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_type(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    326\u001b[0m ):\n\u001b[0;32m--> 327\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    340\u001b[0m         model_name_or_path,\n\u001b[1;32m    341\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m         has_modules\u001b[38;5;241m=\u001b[39mhas_modules,\n\u001b[1;32m    350\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:2286\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_args\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters):\n\u001b[1;32m   2268\u001b[0m     \u001b[38;5;66;03m# Load initialization arguments specific to Transformer-based modules. This includes\u001b[39;00m\n\u001b[1;32m   2269\u001b[0m     \u001b[38;5;66;03m# arguments for loading the model, tokenizer, and configuration, as well as any\u001b[39;00m\n\u001b[1;32m   2270\u001b[0m     \u001b[38;5;66;03m# additional module-specific keyword arguments.\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m     common_transformer_init_kwargs \u001b[38;5;241m=\u001b[39m Transformer\u001b[38;5;241m.\u001b[39m_load_init_kwargs(\n\u001b[1;32m   2272\u001b[0m         model_name_or_path,\n\u001b[1;32m   2273\u001b[0m         \u001b[38;5;66;03m# Loading-specific keyword arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2284\u001b[0m         backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend,\n\u001b[1;32m   2285\u001b[0m     )\n\u001b[0;32m-> 2286\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcommon_transformer_init_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2289\u001b[0m     \u001b[38;5;66;03m# Old modules that don't support the new loading method and don't seem Transformer-based\u001b[39;00m\n\u001b[1;32m   2290\u001b[0m     \u001b[38;5;66;03m# are loaded by downloading the full directories and calling .load() with the old style\u001b[39;00m\n\u001b[1;32m   2291\u001b[0m     \u001b[38;5;66;03m# (i.e. only a path to the local directory)\u001b[39;00m\n\u001b[1;32m   2292\u001b[0m     local_path \u001b[38;5;241m=\u001b[39m load_dir_path(\n\u001b[1;32m   2293\u001b[0m         model_name_or_path\u001b[38;5;241m=\u001b[39mmodel_name_or_path,\n\u001b[1;32m   2294\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39mmodule_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2298\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   2299\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/jina_hyphen_embeddings_hyphen_v3/custom_st.py:82\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adaptation_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     77\u001b[0m     name: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lora_adaptations)\n\u001b[1;32m     78\u001b[0m }\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_task \u001b[38;5;241m=\u001b[39m model_args\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_task\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[1;32m     85\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:586\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_kwargs\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[0;32m--> 586\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# This block handles the case where the user is loading a model with `trust_remote_code=True`\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# but a library model exists with the same name. We don't want to override the autoclass\u001b[39;00m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;66;03m# mappings in this case, or all future loads of that model will be the remote code model.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/transformers/dynamic_module_utils.py:616\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[1;32m    604\u001b[0m final_module \u001b[38;5;241m=\u001b[39m get_cached_module_file(\n\u001b[1;32m    605\u001b[0m     repo_id,\n\u001b[1;32m    606\u001b[0m     module_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    614\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    615\u001b[0m )\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/transformers/dynamic_module_utils.py:299\u001b[0m, in \u001b[0;36mget_class_in_module\u001b[0;34m(class_name, module_path, force_reload)\u001b[0m\n\u001b[1;32m    296\u001b[0m module_spec \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mspec_from_file_location(name, location\u001b[38;5;241m=\u001b[39mmodule_file)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# Hash the module file and all its relative imports to check if we need to reload it\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m module_files: \u001b[38;5;28mlist\u001b[39m[Path] \u001b[38;5;241m=\u001b[39m [module_file] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mmap\u001b[39m(Path, \u001b[43mget_relative_import_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    300\u001b[0m module_hash: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39msha256(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mbytes\u001b[39m(f) \u001b[38;5;241m+\u001b[39m f\u001b[38;5;241m.\u001b[39mread_bytes() \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m module_files))\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[1;32m    302\u001b[0m module: ModuleType\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/transformers/dynamic_module_utils.py:166\u001b[0m, in \u001b[0;36mget_relative_import_files\u001b[0;34m(module_file)\u001b[0m\n\u001b[1;32m    164\u001b[0m new_imports \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files_to_check:\n\u001b[0;32m--> 166\u001b[0m     new_imports\u001b[38;5;241m.\u001b[39mextend(\u001b[43mget_relative_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    168\u001b[0m module_path \u001b[38;5;241m=\u001b[39m Path(module_file)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    169\u001b[0m new_import_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(module_path\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mm)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m new_imports]\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/transformers/dynamic_module_utils.py:135\u001b[0m, in \u001b[0;36mget_relative_imports\u001b[0;34m(module_file)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_relative_imports\u001b[39m(module_file: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    Get the list of modules that are relatively imported in a module file.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m        `list[str]`: The list of relative imports in the module.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    136\u001b[0m         content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Imports of the form `import .xxx`\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/krish/.cache/huggingface/modules/transformers_modules/jinaai/xlm_hyphen_roberta_hyphen_flash_hyphen_implementation/2b6bc3f30750b3a9648fe9b63448c09920efe9be/mha.py'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from transformers import AutoModel\n",
    "\n",
    "# --- 1. HARDWARE OPTIMIZATION ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - Running Ensemble Mode\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# --- 2. FIND TEST DATA ---\n",
    "print(\"üîç Scanning Test Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "if not input_a or not input_b: input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "print(f\"   Track A: {input_a}\\n   Track B: {input_b}\")\n",
    "\n",
    "# --- 3. LOAD THE COUNCIL OF EXPERTS ---\n",
    "print(\"\\nüìÇ Summoning the Models...\")\n",
    "\n",
    "# Expert 1: Logic (DeBERTa) - From Step 1\n",
    "path_deberta = './nli-deberta-v3-large'\n",
    "if os.path.exists(path_deberta):\n",
    "    print(\"   ‚úÖ Loaded Expert 1: Logic (DeBERTa)\")\n",
    "    model_logic = CrossEncoder(path_deberta, device=device)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Local DeBERTa not found. Downloading...\")\n",
    "    model_logic = CrossEncoder('cross-encoder/nli-deberta-v3-large', device=device)\n",
    "\n",
    "# Expert 2: Vibe (RoBERTa STS) - From Step 2\n",
    "path_roberta = './stsb-roberta-large'\n",
    "if os.path.exists(path_roberta):\n",
    "    print(\"   ‚úÖ Loaded Expert 2: Vibe (RoBERTa)\")\n",
    "    model_vibe = CrossEncoder(path_roberta, device=device)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Local RoBERTa not found. Downloading...\")\n",
    "    model_vibe = CrossEncoder('cross-encoder/stsb-roberta-large', device=device)\n",
    "\n",
    "# Expert 3: Narrative Structure (Jina v3) - NEW\n",
    "path_jina = './jina-embeddings-v3'\n",
    "if os.path.exists(path_jina):\n",
    "    print(\"   ‚úÖ Loaded Expert 3: Narrative (Jina v3)\")\n",
    "    # Jina requires trust_remote_code for its special architecture\n",
    "    model_jina = SentenceTransformer(path_jina, trust_remote_code=True, device=device)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Local Jina not found. Downloading...\")\n",
    "    model_jina = SentenceTransformer('jinaai/jina-embeddings-v3', trust_remote_code=True, device=device)\n",
    "\n",
    "# --- 4. EXECUTE TRACK A (ENSEMBLE VOTING) ---\n",
    "print(f\"\\nüß† TRACK A: The Council is Voting...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# Vote 1: Logic Score (Entailment)\n",
    "print(\"   - Asking DeBERTa (Logic)...\")\n",
    "scores_logic_a = model_logic.predict(pairs_a, batch_size=16, show_progress_bar=True)\n",
    "scores_logic_b = model_logic.predict(pairs_b, batch_size=16, show_progress_bar=True)\n",
    "\n",
    "# Fix shape if DeBERTa output 3 columns (Entailment is usually index 1 or 2 depending on version)\n",
    "# nli-deberta-v3-large: [Contradiction, Neutral, Entailment] -> We want Entailment (Index 2) or Neutral+Entailment\n",
    "if len(scores_logic_a.shape) > 1:\n",
    "    # Use Entailment score (Index 1 in 2-class, Index 2 in 3-class)\n",
    "    # nli-deberta-v3-large is usually (Contradiction, Entailment, Neutral) or similar.\n",
    "    # To be safe, we take the LAST column which is usually Entailment.\n",
    "    s_logic_a = scores_logic_a[:, -1]\n",
    "    s_logic_b = scores_logic_b[:, -1]\n",
    "else:\n",
    "    s_logic_a, s_logic_b = scores_logic_a, scores_logic_b\n",
    "\n",
    "# Vote 2: Vibe Score (Similarity)\n",
    "print(\"   - Asking RoBERTa (Vibe)...\")\n",
    "s_vibe_a = model_vibe.predict(pairs_a, batch_size=16, show_progress_bar=True)\n",
    "s_vibe_b = model_vibe.predict(pairs_b, batch_size=16, show_progress_bar=True)\n",
    "\n",
    "# Min-Max Normalization (Crucial to mix them fairly)\n",
    "def normalize(arr):\n",
    "    return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
    "\n",
    "norm_logic_a = normalize(s_logic_a)\n",
    "norm_logic_b = normalize(s_logic_b)\n",
    "norm_vibe_a = normalize(s_vibe_a)\n",
    "norm_vibe_b = normalize(s_vibe_b)\n",
    "\n",
    "# THE FINAL VERDICT (Weighted Average)\n",
    "# We give slightly more weight (0.6) to RoBERTa because \"Similarity\" is the official metric.\n",
    "final_score_a = (0.4 * norm_logic_a) + (0.6 * norm_vibe_a)\n",
    "final_score_b = (0.4 * norm_logic_b) + (0.6 * norm_vibe_b)\n",
    "\n",
    "preds_a = final_score_a > final_score_b\n",
    "\n",
    "# --- 5. EXECUTE TRACK B (TASK SPECIFIC) ---\n",
    "print(f\"\\nüß† TRACK B: Narrative Mapping (Jina)...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    print(\"   - Encoding with task='text-matching'...\")\n",
    "    # Jina v3 supports explicit task instructions\n",
    "    embeddings = model_jina.encode(\n",
    "        df_b[text_col].tolist(),\n",
    "        task=\"text-matching\", # <--- THE MAGIC KEYWORD\n",
    "        batch_size=8, \n",
    "        show_progress_bar=True, \n",
    "        device=device\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. SAVE & ZIP ---\n",
    "print(\"\\nüì¶ Packaging the Ensemble Result...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_ENSEMBLE_JINA.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ VICTORY! Upload '{zip_name}' to CodaBench.\")\n",
    "print(\"   This submission combines Logic + Vibes + Narrative Structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf71a0e3-efef-403a-a7a7-5399880f83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krish/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/krish/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple M4 (MPS) - Safe Mode\n",
      "\n",
      "üîç Scanning Data...\n",
      "   Track A: test_track_a.jsonl\n",
      "   Track B: test_track_b.jsonl\n",
      "\n",
      "üìÇ Loading Models...\n",
      "   1. Loading DeBERTa (Logic)...\n",
      "   2. Loading RoBERTa (Vibe)...\n",
      "   3. Loading Jina v3 (Narrative)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† TRACK A: Voting Process...\n",
      "   - Asking DeBERTa (Logic) [Batch Size: 1]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [01:35<00:00,  4.20it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [01:26<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Asking RoBERTa (Vibe) [Batch Size: 8]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:17<00:00,  1.55s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:10<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† TRACK B: Narrative Mapping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [40:15<00:00, 11.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Zipping Submission...\n",
      "\n",
      "üèÜ SUCCESS! Upload 'submission_ENSEMBLE_SAFE.zip' to CodaBench.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "\n",
    "# --- 1. HARDWARE SETUP ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - Safe Mode\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# --- 2. FIND DATA ---\n",
    "print(\"\\nüîç Scanning Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "if not input_a or not input_b: input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "print(f\"   Track A: {input_a}\\n   Track B: {input_b}\")\n",
    "\n",
    "# --- 3. LOAD MODELS (ONE BY ONE TO SAVE RAM) ---\n",
    "print(\"\\nüìÇ Loading Models...\")\n",
    "\n",
    "# Expert 1: DeBERTa (Logic) - THE MEMORY HOG\n",
    "print(\"   1. Loading DeBERTa (Logic)...\")\n",
    "model_logic = CrossEncoder('cross-encoder/nli-deberta-v3-large', device=device)\n",
    "\n",
    "# Expert 2: RoBERTa (Vibe)\n",
    "print(\"   2. Loading RoBERTa (Vibe)...\")\n",
    "model_vibe = CrossEncoder('cross-encoder/stsb-roberta-large', device=device)\n",
    "\n",
    "# Expert 3: Jina v3 (Narrative)\n",
    "print(\"   3. Loading Jina v3 (Narrative)...\")\n",
    "model_jina = SentenceTransformer('jinaai/jina-embeddings-v3', trust_remote_code=True, device=device)\n",
    "\n",
    "# --- 4. RUN TRACK A (SAFE BATCH SIZES) ---\n",
    "print(\"\\nüß† TRACK A: Voting Process...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# VOTE 1: LOGIC (Batch Size = 1 is CRITICAL here)\n",
    "print(\"   - Asking DeBERTa (Logic) [Batch Size: 1]...\")\n",
    "# We use batch_size=1 to prevent M4 memory freeze\n",
    "s_logic_a = model_logic.predict(pairs_a, batch_size=1, show_progress_bar=True)\n",
    "s_logic_b = model_logic.predict(pairs_b, batch_size=1, show_progress_bar=True)\n",
    "\n",
    "if len(s_logic_a.shape) > 1: s_logic_a, s_logic_b = s_logic_a[:, -1], s_logic_b[:, -1]\n",
    "\n",
    "# VOTE 2: VIBE (Batch Size = 8 is fine here)\n",
    "print(\"   - Asking RoBERTa (Vibe) [Batch Size: 8]...\")\n",
    "s_vibe_a = model_vibe.predict(pairs_a, batch_size=8, show_progress_bar=True)\n",
    "s_vibe_b = model_vibe.predict(pairs_b, batch_size=8, show_progress_bar=True)\n",
    "\n",
    "# Normalize & Vote\n",
    "def normalize(arr): return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
    "n_logic_a, n_logic_b = normalize(s_logic_a), normalize(s_logic_b)\n",
    "n_vibe_a, n_vibe_b = normalize(s_vibe_a), normalize(s_vibe_b)\n",
    "\n",
    "final_a = (0.4 * n_logic_a) + (0.6 * n_vibe_a)\n",
    "final_b = (0.4 * n_logic_b) + (0.6 * n_vibe_b)\n",
    "preds_a = final_a > final_b\n",
    "\n",
    "# --- 5. RUN TRACK B (JINA) ---\n",
    "print(\"\\nüß† TRACK B: Narrative Mapping...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    embeddings = model_jina.encode(\n",
    "        df_b[text_col].tolist(),\n",
    "        task=\"text-matching\",\n",
    "        batch_size=4, # Keep this moderate\n",
    "        show_progress_bar=True,\n",
    "        device=device\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. SAVE & ZIP ---\n",
    "print(\"\\nüì¶ Zipping Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_ENSEMBLE_SAFE.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ SUCCESS! Upload '{zip_name}' to CodaBench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b12e10f-b6d3-4873-a863-e3375cc57598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krish/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/krish/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple M4 (MPS) - Single-File Mode\n",
      "üîç Scanning Data...\n",
      "   Track A: test_track_a.jsonl\n",
      "   Track B: test_track_b.jsonl\n",
      "\n",
      "üìÇ Loading Models...\n",
      "   ‚úÖ Loading Jina Reranker (Track A)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './jina-reranker-v2' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Loading Jina Embedder (Track B)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './jina-embeddings-v3' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "The tokenizer you are loading from './jina-embeddings-v3' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† TRACK A: Reading full stories (Progress Bar Enabled)...\n",
      "   - Scoring Pair A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Scoring: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:20<00:00, 19.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Scoring Pair B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Scoring: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:18<00:00, 21.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† TRACK B: Narrative Mapping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 849/849 [06:35<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Zipping Submission...\n",
      "\n",
      "üèÜ READY! Upload 'submission_FINAL_SAFE.zip' to CodaBench.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm  # Progress bar library\n",
    "\n",
    "# --- 1. HARDWARE ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - Single-File Mode\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# --- 2. FIND DATA ---\n",
    "print(\"üîç Scanning Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "if not input_a or not input_b: input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "print(f\"   Track A: {input_a}\\n   Track B: {input_b}\")\n",
    "\n",
    "# --- 3. LOAD MODELS ---\n",
    "print(\"\\nüìÇ Loading Models...\")\n",
    "\n",
    "# TRACK A: Jina Reranker\n",
    "path_rerank = './jina-reranker-v2'\n",
    "if os.path.exists(path_rerank):\n",
    "    print(\"   ‚úÖ Loading Jina Reranker (Track A)...\")\n",
    "    model_a = AutoModelForSequenceClassification.from_pretrained(\n",
    "        path_rerank, \n",
    "        trust_remote_code=True, \n",
    "        torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_a = AutoTokenizer.from_pretrained(path_rerank, trust_remote_code=True)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Local Reranker not found. Downloading...\")\n",
    "    model_a = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'jinaai/jina-reranker-v2-base-multilingual', \n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_a = AutoTokenizer.from_pretrained('jinaai/jina-reranker-v2-base-multilingual', trust_remote_code=True)\n",
    "\n",
    "# TRACK B: Jina Embeddings\n",
    "path_embed = './jina-embeddings-v3'\n",
    "if os.path.exists(path_embed):\n",
    "    print(\"   ‚úÖ Loading Jina Embedder (Track B)...\")\n",
    "    model_b = SentenceTransformer(path_embed, trust_remote_code=True, device=device)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Local Embedder not found. Downloading...\")\n",
    "    model_b = SentenceTransformer('jinaai/jina-embeddings-v3', trust_remote_code=True, device=device)\n",
    "\n",
    "# --- 4. EXECUTE TRACK A (SAFE MODE) ---\n",
    "print(f\"\\nüß† TRACK A: Reading full stories (Progress Bar Enabled)...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "def predict_safe(pairs, model, tokenizer):\n",
    "    scores = []\n",
    "    # Batch size 1 = Maximum Safety for Memory\n",
    "    for i in tqdm(range(0, len(pairs), 1), desc=\"   - Scoring\"): \n",
    "        batch = pairs[i:i+1]\n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=1024, # Good context length\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs).logits.squeeze(-1)\n",
    "            scores.extend(output.cpu().float().numpy())\n",
    "    return scores\n",
    "\n",
    "print(\"   - Scoring Pair A...\")\n",
    "scores_a = predict_safe(pairs_a, model_a, tokenizer_a)\n",
    "print(\"   - Scoring Pair B...\")\n",
    "scores_b = predict_safe(pairs_b, model_a, tokenizer_a)\n",
    "\n",
    "preds_a = [s_a > s_b for s_a, s_b in zip(scores_a, scores_b)]\n",
    "\n",
    "# --- 5. EXECUTE TRACK B (SAFE MODE) ---\n",
    "print(f\"\\nüß† TRACK B: Narrative Mapping...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    # Set max length on the model object\n",
    "    model_b.max_seq_length = 2048 \n",
    "    \n",
    "    embeddings = model_b.encode(\n",
    "        df_b[text_col].tolist(),\n",
    "        task=\"text-matching\", \n",
    "        batch_size=1, # Reduced to 1 for safety\n",
    "        show_progress_bar=True,\n",
    "        device=device\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. SAVE & ZIP ---\n",
    "print(\"\\nüì¶ Zipping Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_FINAL_SAFE.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ READY! Upload '{zip_name}' to CodaBench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2f92ad-83b2-4952-a66c-a77196b6c035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple M4 (MPS) - Ensemble Mode\n",
      "üîç Scanning Data...\n",
      "   Track A: test_track_a.jsonl\n",
      "   Track B: test_track_b.jsonl\n",
      "\n",
      "üìÇ Loading Models...\n",
      "   ‚úÖ Expert 1: Jina Reranker (Loaded from local)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './jina-reranker-v2' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Expert 2: RoBERTa STS (Loaded from local)\n",
      "   ‚úÖ Expert 3: Jina Embeddings (Loaded from local)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './jina-embeddings-v3' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "The tokenizer you are loading from './jina-embeddings-v3' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† TRACK A: The Ensemble is Voting...\n",
      "   - Collecting Jina Scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Jina Voting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [1:15:59<00:00, 11.40s/it]\n",
      "   - Jina Voting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [1:14:30<00:00, 11.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Collecting RoBERTa Scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:14<00:00,  1.33it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:00<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† TRACK B: Deep Context Embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 849/849 [08:11<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Zipping Submission...\n",
      "\n",
      "üèÜ READY! Upload 'submission_ENSEMBLE_FINAL.zip' to CodaBench.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. HARDWARE ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - Ensemble Mode\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# --- 2. FIND DATA ---\n",
    "print(\"üîç Scanning Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "if not input_a or not input_b: input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "print(f\"   Track A: {input_a}\\n   Track B: {input_b}\")\n",
    "\n",
    "# --- 3. LOAD THE TEAM OF EXPERTS ---\n",
    "print(\"\\nüìÇ Loading Models...\")\n",
    "\n",
    "# Expert 1: Jina Reranker (Long Context)\n",
    "path_jina_rerank = './jina-reranker-v2'\n",
    "if os.path.exists(path_jina_rerank):\n",
    "    print(\"   ‚úÖ Expert 1: Jina Reranker (Loaded from local)\")\n",
    "    model_jina = AutoModelForSequenceClassification.from_pretrained(\n",
    "        path_jina_rerank, trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_jina = AutoTokenizer.from_pretrained(path_jina_rerank, trust_remote_code=True)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Expert 1 not found locally. Downloading...\")\n",
    "    model_jina = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'jinaai/jina-reranker-v2-base-multilingual', trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_jina = AutoTokenizer.from_pretrained('jinaai/jina-reranker-v2-base-multilingual', trust_remote_code=True)\n",
    "\n",
    "# Expert 2: RoBERTa STS (Semantic Similarity)\n",
    "path_roberta = './stsb-roberta-large'\n",
    "if os.path.exists(path_roberta):\n",
    "    print(\"   ‚úÖ Expert 2: RoBERTa STS (Loaded from local)\")\n",
    "    model_roberta = CrossEncoder(path_roberta, device=device)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Expert 2 not found locally. Downloading...\")\n",
    "    model_roberta = CrossEncoder('cross-encoder/stsb-roberta-large', device=device)\n",
    "\n",
    "# Expert 3: Jina Embeddings (Track B)\n",
    "path_jina_embed = './jina-embeddings-v3'\n",
    "if os.path.exists(path_jina_embed):\n",
    "    print(\"   ‚úÖ Expert 3: Jina Embeddings (Loaded from local)\")\n",
    "    model_embed = SentenceTransformer(path_jina_embed, trust_remote_code=True, device=device)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Expert 3 not found locally. Downloading...\")\n",
    "    model_embed = SentenceTransformer('jinaai/jina-embeddings-v3', trust_remote_code=True, device=device)\n",
    "\n",
    "# --- 4. EXECUTE TRACK A (THE CONSENSUS) ---\n",
    "print(f\"\\nüß† TRACK A: The Ensemble is Voting...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# -- VOTE 1: JINA RERANKER --\n",
    "def predict_jina(pairs):\n",
    "    scores = []\n",
    "    for i in tqdm(range(len(pairs)), desc=\"   - Jina Voting\"):\n",
    "        batch = pairs[i:i+1]\n",
    "        inputs = tokenizer_jina(\n",
    "            batch, padding=True, truncation=True, max_length=1024, return_tensors='pt'\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model_jina(**inputs).logits.squeeze(-1)\n",
    "            scores.extend(output.cpu().float().numpy())\n",
    "    return scores\n",
    "\n",
    "print(\"   - Collecting Jina Scores...\")\n",
    "jina_a = predict_jina(pairs_a)\n",
    "jina_b = predict_jina(pairs_b)\n",
    "\n",
    "# -- VOTE 2: ROBERTA STS --\n",
    "# RoBERTa is smaller, so we can use slightly larger batch, but let's keep it safe at 4\n",
    "print(\"   - Collecting RoBERTa Scores...\")\n",
    "roberta_a = model_roberta.predict(pairs_a, batch_size=4, show_progress_bar=True)\n",
    "roberta_b = model_roberta.predict(pairs_b, batch_size=4, show_progress_bar=True)\n",
    "\n",
    "# -- MERGING VOTES (NORMALIZATION) --\n",
    "def normalize(arr):\n",
    "    arr = np.array(arr)\n",
    "    return (arr - arr.min()) / (arr.max() - arr.min())\n",
    "\n",
    "# Normalize both so they are on the same 0.0 to 1.0 scale\n",
    "norm_jina_a, norm_jina_b = normalize(jina_a), normalize(jina_b)\n",
    "norm_rob_a, norm_rob_b = normalize(roberta_a), normalize(roberta_b)\n",
    "\n",
    "# Weighted Average: 60% Jina (Context) + 40% RoBERTa (Similarity)\n",
    "final_a = (0.6 * norm_jina_a) + (0.4 * norm_rob_a)\n",
    "final_b = (0.6 * norm_jina_b) + (0.4 * norm_rob_b)\n",
    "\n",
    "preds_a = final_a > final_b\n",
    "\n",
    "# --- 5. EXECUTE TRACK B (DEEP READ) ---\n",
    "print(f\"\\nüß† TRACK B: Deep Context Embedding...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    # Boost context to 4096 to ensure we catch every outcome\n",
    "    model_embed.max_seq_length = 4096\n",
    "    \n",
    "    embeddings = model_embed.encode(\n",
    "        df_b[text_col].tolist(),\n",
    "        task=\"text-matching\",\n",
    "        batch_size=1, # Safety first\n",
    "        show_progress_bar=True,\n",
    "        device=device\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. ZIP AND SHIP ---\n",
    "print(\"\\nüì¶ Zipping Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_ENSEMBLE_FINAL.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ READY! Upload '{zip_name}' to CodaBench.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d98ecb-63d3-4a4b-a04c-560bece7e7a5",
   "metadata": {},
   "source": [
    "## import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. HARDWARE ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - God Mode\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# --- 2. FIND DATA ---\n",
    "print(\"üîç Scanning Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "if not input_a or not input_b: input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "print(f\"   Track A: {input_a}\\n   Track B: {input_b}\")\n",
    "\n",
    "# --- 3. LOAD THE CHAMPIONS ---\n",
    "print(\"\\nüìÇ Loading Models...\")\n",
    "\n",
    "# Expert 1: Jina Reranker (The Long-Context Reader)\n",
    "path_jina = './jina-reranker-v2'\n",
    "if os.path.exists(path_jina):\n",
    "    print(\"   ‚úÖ Expert 1: Jina Reranker v2 (Local)\")\n",
    "    model_jina = AutoModelForSequenceClassification.from_pretrained(\n",
    "        path_jina, trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_jina = AutoTokenizer.from_pretrained(path_jina, trust_remote_code=True)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Jina not found locally. Downloading...\")\n",
    "    model_jina = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'jinaai/jina-reranker-v2-base-multilingual', trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_jina = AutoTokenizer.from_pretrained('jinaai/jina-reranker-v2-base-multilingual', trust_remote_code=True)\n",
    "\n",
    "# Expert 2: BGE-M3 Reranker (The Modern Judge)\n",
    "path_bge = './bge-reranker-v2-m3'\n",
    "if os.path.exists(path_bge):\n",
    "    print(\"   ‚úÖ Expert 2: BGE-M3 (Local)\")\n",
    "    model_bge = AutoModelForSequenceClassification.from_pretrained(\n",
    "        path_bge, trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_bge = AutoTokenizer.from_pretrained(path_bge, trust_remote_code=True)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è BGE-M3 not found locally. Downloading...\")\n",
    "    model_bge = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'BAAI/bge-reranker-v2-m3', trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_bge = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3', trust_remote_code=True)\n",
    "\n",
    "# Expert 3: Jina Embeddings (Track B)\n",
    "path_embed = './jina-embeddings-v3'\n",
    "if os.path.exists(path_embed):\n",
    "    print(\"   ‚úÖ Expert 3: Jina Embeddings v3 (Local)\")\n",
    "    model_embed = SentenceTransformer(path_embed, trust_remote_code=True, device=device)\n",
    "else:\n",
    "    model_embed = SentenceTransformer('jinaai/jina-embeddings-v3', trust_remote_code=True, device=device)\n",
    "\n",
    "# --- 4. EXECUTE TRACK A (THE HYBRID VOTE) ---\n",
    "print(f\"\\nüß† TRACK A: Consensus Voting...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# Helper for Inference\n",
    "def predict_reranker(pairs, model, tokenizer, name, max_len=1024):\n",
    "    scores = []\n",
    "    for i in tqdm(range(len(pairs)), desc=f\"   - {name}\"):\n",
    "        batch = pairs[i:i+1]\n",
    "        inputs = tokenizer(\n",
    "            batch, padding=True, truncation=True, max_length=max_len, return_tensors='pt'\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs).logits.squeeze(-1)\n",
    "            scores.extend(output.cpu().float().numpy())\n",
    "    return scores\n",
    "\n",
    "# Vote 1: Jina (Reads 1024 tokens)\n",
    "jina_a = predict_reranker(pairs_a, model_jina, tokenizer_jina, \"Jina\", 1024)\n",
    "jina_b = predict_reranker(pairs_b, model_jina, tokenizer_jina, \"Jina\", 1024)\n",
    "\n",
    "# Vote 2: BGE-M3 (Reads 1024 tokens - Higher Capacity)\n",
    "bge_a = predict_reranker(pairs_a, model_bge, tokenizer_bge, \"BGE-M3\", 1024)\n",
    "bge_b = predict_reranker(pairs_b, model_bge, tokenizer_bge, \"BGE-M3\", 1024)\n",
    "\n",
    "# Normalize and Merge\n",
    "def normalize(arr):\n",
    "    arr = np.array(arr)\n",
    "    return (arr - arr.min()) / (arr.max() - arr.min())\n",
    "\n",
    "n_jina_a, n_jina_b = normalize(jina_a), normalize(jina_b)\n",
    "n_bge_a, n_bge_b = normalize(bge_a), normalize(bge_b)\n",
    "\n",
    "# 50/50 Split - Both models are SOTA\n",
    "final_a = (0.5 * n_jina_a) + (0.5 * n_bge_a)\n",
    "final_b = (0.5 * n_jina_b) + (0.5 * n_bge_b)\n",
    "preds_a = final_a > final_b\n",
    "\n",
    "# --- 5. EXECUTE TRACK B (RICH PROMPT) ---\n",
    "print(f\"\\nüß† TRACK B: Deep Narrative Embedding...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    # 1. Max Context\n",
    "    model_embed.max_seq_length = 4096\n",
    "    \n",
    "    # 2. RICH PROMPT (The \"Insane\" Tweak)\n",
    "    # Instead of just \"text-matching\", we guide the model to look for themes.\n",
    "    # We apply this prompt to every story before embedding.\n",
    "    rich_prompt = \"Retrieve stories with similar abstract themes, course of action, and outcomes: \"\n",
    "    texts = [rich_prompt + t for t in df_b[text_col].tolist()]\n",
    "\n",
    "    embeddings = model_embed.encode(\n",
    "        texts,\n",
    "        task=\"text-matching\", \n",
    "        batch_size=1, \n",
    "        show_progress_bar=True,\n",
    "        device=device\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. ZIP AND SHIP ---\n",
    "print(\"\\nüì¶ Zipping Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_GOD_MODE.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ VICTORY! Upload '{zip_name}' to CodaBench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea62b1e-26d1-4128-9369-b1829f83e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krish/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/krish/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple M4 (MPS) - God Mode\n",
      "üîç Scanning Data...\n",
      "   Track A: test_track_a.jsonl\n",
      "   Track B: test_track_b.jsonl\n",
      "\n",
      "üìÇ Loading Models...\n",
      "   ‚úÖ Expert 1: Jina Reranker v2 (Local)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './jina-reranker-v2' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Expert 2: BGE-M3 (Local)\n",
      "   ‚úÖ Expert 3: Jina Embeddings v3 (Local)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './jina-embeddings-v3' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "The tokenizer you are loading from './jina-embeddings-v3' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† TRACK A: Consensus Voting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Jina: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:18<00:00, 21.30it/s]\n",
      "   - Jina: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:17<00:00, 22.80it/s]\n",
      "   - BGE-M3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:47<00:00,  8.37it/s]\n",
      "   - BGE-M3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:44<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† TRACK B: Deep Narrative Embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 849/849 [06:33<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Zipping Submission...\n",
      "\n",
      "üèÜ VICTORY! Upload 'submission_GOD_MODE.zip' to CodaBench.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. HARDWARE ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - God Mode\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# --- 2. FIND DATA ---\n",
    "print(\"üîç Scanning Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "if not input_a or not input_b: input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "print(f\"   Track A: {input_a}\\n   Track B: {input_b}\")\n",
    "\n",
    "# --- 3. LOAD THE CHAMPIONS ---\n",
    "print(\"\\nüìÇ Loading Models...\")\n",
    "\n",
    "# Expert 1: Jina Reranker (The Long-Context Reader)\n",
    "path_jina = './jina-reranker-v2'\n",
    "if os.path.exists(path_jina):\n",
    "    print(\"   ‚úÖ Expert 1: Jina Reranker v2 (Local)\")\n",
    "    model_jina = AutoModelForSequenceClassification.from_pretrained(\n",
    "        path_jina, trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_jina = AutoTokenizer.from_pretrained(path_jina, trust_remote_code=True)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Jina not found locally. Downloading...\")\n",
    "    model_jina = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'jinaai/jina-reranker-v2-base-multilingual', trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_jina = AutoTokenizer.from_pretrained('jinaai/jina-reranker-v2-base-multilingual', trust_remote_code=True)\n",
    "\n",
    "# Expert 2: BGE-M3 Reranker (The Modern Judge)\n",
    "path_bge = './bge-reranker-v2-m3'\n",
    "if os.path.exists(path_bge):\n",
    "    print(\"   ‚úÖ Expert 2: BGE-M3 (Local)\")\n",
    "    model_bge = AutoModelForSequenceClassification.from_pretrained(\n",
    "        path_bge, trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_bge = AutoTokenizer.from_pretrained(path_bge, trust_remote_code=True)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è BGE-M3 not found locally. Downloading...\")\n",
    "    model_bge = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'BAAI/bge-reranker-v2-m3', trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_bge = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3', trust_remote_code=True)\n",
    "\n",
    "# Expert 3: Jina Embeddings (Track B)\n",
    "path_embed = './jina-embeddings-v3'\n",
    "if os.path.exists(path_embed):\n",
    "    print(\"   ‚úÖ Expert 3: Jina Embeddings v3 (Local)\")\n",
    "    model_embed = SentenceTransformer(path_embed, trust_remote_code=True, device=device)\n",
    "else:\n",
    "    model_embed = SentenceTransformer('jinaai/jina-embeddings-v3', trust_remote_code=True, device=device)\n",
    "\n",
    "# --- 4. EXECUTE TRACK A (THE HYBRID VOTE) ---\n",
    "print(f\"\\nüß† TRACK A: Consensus Voting...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# Helper for Inference\n",
    "def predict_reranker(pairs, model, tokenizer, name, max_len=1024):\n",
    "    scores = []\n",
    "    for i in tqdm(range(len(pairs)), desc=f\"   - {name}\"):\n",
    "        batch = pairs[i:i+1]\n",
    "        inputs = tokenizer(\n",
    "            batch, padding=True, truncation=True, max_length=max_len, return_tensors='pt'\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs).logits.squeeze(-1)\n",
    "            scores.extend(output.cpu().float().numpy())\n",
    "    return scores\n",
    "\n",
    "# Vote 1: Jina (Reads 1024 tokens)\n",
    "jina_a = predict_reranker(pairs_a, model_jina, tokenizer_jina, \"Jina\", 1024)\n",
    "jina_b = predict_reranker(pairs_b, model_jina, tokenizer_jina, \"Jina\", 1024)\n",
    "\n",
    "# Vote 2: BGE-M3 (Reads 1024 tokens - Higher Capacity)\n",
    "bge_a = predict_reranker(pairs_a, model_bge, tokenizer_bge, \"BGE-M3\", 1024)\n",
    "bge_b = predict_reranker(pairs_b, model_bge, tokenizer_bge, \"BGE-M3\", 1024)\n",
    "\n",
    "# Normalize and Merge\n",
    "def normalize(arr):\n",
    "    arr = np.array(arr)\n",
    "    return (arr - arr.min()) / (arr.max() - arr.min())\n",
    "\n",
    "n_jina_a, n_jina_b = normalize(jina_a), normalize(jina_b)\n",
    "n_bge_a, n_bge_b = normalize(bge_a), normalize(bge_b)\n",
    "\n",
    "# 50/50 Split - Both models are SOTA\n",
    "final_a = (0.5 * n_jina_a) + (0.5 * n_bge_a)\n",
    "final_b = (0.5 * n_jina_b) + (0.5 * n_bge_b)\n",
    "preds_a = final_a > final_b\n",
    "\n",
    "# --- 5. EXECUTE TRACK B (RICH PROMPT) ---\n",
    "print(f\"\\nüß† TRACK B: Deep Narrative Embedding...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    # 1. Max Context\n",
    "    model_embed.max_seq_length = 4096\n",
    "    \n",
    "    # 2. RICH PROMPT (The \"Insane\" Tweak)\n",
    "    # Instead of just \"text-matching\", we guide the model to look for themes.\n",
    "    # We apply this prompt to every story before embedding.\n",
    "    rich_prompt = \"Retrieve stories with similar abstract themes, course of action, and outcomes: \"\n",
    "    texts = [rich_prompt + t for t in df_b[text_col].tolist()]\n",
    "\n",
    "    embeddings = model_embed.encode(\n",
    "        texts,\n",
    "        task=\"text-matching\", \n",
    "        batch_size=1, \n",
    "        show_progress_bar=True,\n",
    "        device=device\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. ZIP AND SHIP ---\n",
    "print(\"\\nüì¶ Zipping Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_GOD_MODE.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ VICTORY! Upload '{zip_name}' to CodaBench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6c5c76-9857-4ad1-a97e-ebf1e1ebc506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Inspecting submission_GOD_MODE.zip...\n",
      "   üìÇ Files inside: ['track_a.jsonl', 'track_b.jsonl']\n",
      "   ‚úÖ Structure: Flat (Good)\n",
      "   ‚úÖ Track A Lines: 400 (Should be 400)\n",
      "   ‚úÖ Track B Lines: 849 (Should be 849)\n",
      "\n",
      "üöÄ If all checks are Green, the file is valid.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import os\n",
    "\n",
    "zip_name = 'submission_GOD_MODE.zip' # Make sure this matches your file name\n",
    "\n",
    "print(f\"üîç Inspecting {zip_name}...\")\n",
    "\n",
    "if not os.path.exists(zip_name):\n",
    "    print(\"‚ùå Error: Zip file not found!\")\n",
    "else:\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_name, 'r') as z:\n",
    "            files = z.namelist()\n",
    "            print(f\"   üìÇ Files inside: {files}\")\n",
    "            \n",
    "            # CHECK 1: Are files at the root?\n",
    "            if any('/' in f for f in files):\n",
    "                print(\"   ‚ùå CRITICAL: Zip contains folders! CodaBench needs files at the root.\")\n",
    "            else:\n",
    "                print(\"   ‚úÖ Structure: Flat (Good)\")\n",
    "\n",
    "            # CHECK 2: Track A\n",
    "            if 'track_a.jsonl' in files:\n",
    "                with z.open('track_a.jsonl') as f:\n",
    "                    lines = f.readlines()\n",
    "                    print(f\"   ‚úÖ Track A Lines: {len(lines)} (Should be 400)\")\n",
    "                    first = json.loads(lines[0])\n",
    "                    if \"text_a_is_closer\" not in first:\n",
    "                        print(f\"   ‚ùå Track A Key Error: Found {first.keys()}\")\n",
    "            else:\n",
    "                print(\"   ‚ùå Track A missing!\")\n",
    "\n",
    "            # CHECK 3: Track B\n",
    "            if 'track_b.jsonl' in files:\n",
    "                with z.open('track_b.jsonl') as f:\n",
    "                    lines = f.readlines()\n",
    "                    print(f\"   ‚úÖ Track B Lines: {len(lines)} (Should be 849)\")\n",
    "                    first = json.loads(lines[0])\n",
    "                    if \"embedding\" not in first:\n",
    "                        print(f\"   ‚ùå Track B Key Error: Found {first.keys()}\")\n",
    "                    if len(first['embedding']) != 1024:\n",
    "                        print(f\"   ‚ö†Ô∏è Track B Dim: {len(first['embedding'])} (Jina V3 usually 1024)\")\n",
    "            else:\n",
    "                print(\"   ‚ùå Track B missing!\")\n",
    "\n",
    "            print(\"\\nüöÄ If all checks are Green, the file is valid.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Corrupt Zip: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9036e4e-c77c-4036-be5b-01926cfc47d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple M4 (MPS) - Final Fix Mode\n",
      "üîç Scanning Data...\n",
      "   Track A: test_track_a.jsonl\n",
      "   Track B: test_track_b.jsonl\n",
      "\n",
      "üõ†Ô∏è Hacking Model Configuration on Disk...\n",
      "   - Disabled 'use_memory_efficient_attention'\n",
      "   - Disabled 'unpad_inputs'\n",
      "   ‚úÖ Config file patched successfully!\n",
      "\n",
      "üìÇ Loading Specialists...\n",
      "   ‚úÖ Expert 1: BGE-M3 (Local)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./stella_en_400M_v5 were not used when initializing NewModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Expert 2: Stella v5 (Local & Patched)\n",
      "\n",
      "üß† TRACK A: Precision Judging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - BGE Scoring: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [33:10<00:00,  4.98s/it]\n",
      "   - BGE Scoring: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [40:20<00:00,  6.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† TRACK B: Narrative Instruction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|                                          | 0/849 [00:00<?, ?it/s]/Users/krish/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:1621: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 849/849 [13:52<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Zipping Submission...\n",
      "\n",
      "üèÜ READY! Upload 'submission_STELLA_FINAL.zip' to CodaBench.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. HARDWARE ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - Final Fix Mode\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# --- 2. FIND DATA ---\n",
    "print(\"üîç Scanning Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "if not input_a or not input_b: input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "print(f\"   Track A: {input_a}\\n   Track B: {input_b}\")\n",
    "\n",
    "# --- 3. THE \"CONFIG HACK\" (CRITICAL FIX) ---\n",
    "print(\"\\nüõ†Ô∏è Hacking Model Configuration on Disk...\")\n",
    "stella_path = './stella_en_400M_v5'\n",
    "config_file = os.path.join(stella_path, 'config.json')\n",
    "\n",
    "if os.path.exists(config_file):\n",
    "    with open(config_file, 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "    \n",
    "    # FORCE DISABLE the crash-causing settings\n",
    "    changed = False\n",
    "    if config_data.get('use_memory_efficient_attention') is not False:\n",
    "        config_data['use_memory_efficient_attention'] = False\n",
    "        changed = True\n",
    "        print(\"   - Disabled 'use_memory_efficient_attention'\")\n",
    "        \n",
    "    if config_data.get('unpad_inputs') is not False:\n",
    "        config_data['unpad_inputs'] = False\n",
    "        changed = True\n",
    "        print(\"   - Disabled 'unpad_inputs'\")\n",
    "        \n",
    "    if changed:\n",
    "        with open(config_file, 'w') as f:\n",
    "            json.dump(config_data, f, indent=2)\n",
    "        print(\"   ‚úÖ Config file patched successfully!\")\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è Config was already patched.\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Warning: Config file not found at {config_file}. Model might crash if downloading fresh.\")\n",
    "\n",
    "# --- 4. LOAD MODELS ---\n",
    "print(\"\\nüìÇ Loading Specialists...\")\n",
    "\n",
    "# Expert 1: BGE-M3 (Track A)\n",
    "path_bge = './bge-reranker-v2-m3'\n",
    "if os.path.exists(path_bge):\n",
    "    print(\"   ‚úÖ Expert 1: BGE-M3 (Local)\")\n",
    "    model_a = AutoModelForSequenceClassification.from_pretrained(\n",
    "        path_bge, trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_a = AutoTokenizer.from_pretrained(path_bge, trust_remote_code=True)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è BGE-M3 not found. Downloading...\")\n",
    "    model_a = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'BAAI/bge-reranker-v2-m3', trust_remote_code=True, torch_dtype=torch.float16\n",
    "    ).to(device).eval()\n",
    "    tokenizer_a = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3', trust_remote_code=True)\n",
    "\n",
    "# Expert 2: Stella v5 (Track B)\n",
    "# Now that config.json is fixed, we can load it normally!\n",
    "if os.path.exists(stella_path):\n",
    "    print(\"   ‚úÖ Expert 2: Stella v5 (Local & Patched)\")\n",
    "    model_b = SentenceTransformer(stella_path, trust_remote_code=True, device=device)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Stella not found locally. Downloading (Might crash on first run)...\")\n",
    "    model_b = SentenceTransformer('dunzhang/stella_en_400M_v5', trust_remote_code=True, device=device)\n",
    "\n",
    "# --- 5. EXECUTE TRACK A (BGE-M3) ---\n",
    "print(f\"\\nüß† TRACK A: Precision Judging...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "def predict_bge(pairs):\n",
    "    scores = []\n",
    "    for i in tqdm(range(len(pairs)), desc=\"   - BGE Scoring\"):\n",
    "        batch = pairs[i:i+1]\n",
    "        inputs = tokenizer_a(\n",
    "            batch, padding=True, truncation=True, max_length=1024, return_tensors='pt'\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model_a(**inputs).logits.squeeze(-1)\n",
    "            scores.extend(output.cpu().float().numpy())\n",
    "    return scores\n",
    "\n",
    "scores_a = predict_bge(pairs_a)\n",
    "scores_b = predict_bge(pairs_b)\n",
    "preds_a = [s_a > s_b for s_a, s_b in zip(scores_a, scores_b)]\n",
    "\n",
    "# --- 6. EXECUTE TRACK B (STELLA) ---\n",
    "print(f\"\\nüß† TRACK B: Narrative Instruction...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    # Stella specific prompting\n",
    "    narrative_prompt = \"Retrieve a story that shares the same abstract theme, narrative flow, and final outcome.\"\n",
    "    embeddings = model_b.encode(\n",
    "        df_b[text_col].tolist(),\n",
    "        prompt=narrative_prompt,\n",
    "        batch_size=1, \n",
    "        show_progress_bar=True,\n",
    "        device=device\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 7. SAVE & ZIP ---\n",
    "print(\"\\nüì¶ Zipping Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_STELLA_FINAL.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ READY! Upload '{zip_name}' to CodaBench.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
