{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d9979d-bbeb-400e-a494-3eb067c2a391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ POWER UNLEASHED: Using Apple M4 (MPS) Acceleration\n",
      "\n",
      "üîç Scanning for Test Data...\n",
      "   ‚úÖ Track A File: test_track_a.jsonl\n",
      "   ‚úÖ Track B File: test_track_b.jsonl\n",
      "\n",
      "‚¨áÔ∏è STARTING DOWNLOADS (This ensures you see progress)...\n",
      "   1. Downloading DeBERTa-v3-Large (~800MB)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 17 files:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 5/17 [26:16<1:03:03, 315.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                   initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:608\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:440\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   1. Downloading DeBERTa-v3-Large (~800MB)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m model_a_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross-encoder/nli-deberta-v3-large\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 43\u001b[0m \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_a_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# This triggers the bar\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Model 2: GTE Large\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   2. Downloading GTE-Large-v1.5 (~1.5GB)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py:332\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[1;32m    330\u001b[0m         _inner_hf_hub_download(file)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(local_dir))\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SemEval_Task4/venv/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tqdm_class(ex\u001b[38;5;241m.\u001b[39mmap(fn, \u001b[38;5;241m*\u001b[39miterables, chunksize\u001b[38;5;241m=\u001b[39mchunksize), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:636\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 636\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py:229\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 229\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:1053\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:1069\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1070\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from huggingface_hub import snapshot_download # This helps us show download bars\n",
    "import torch\n",
    "\n",
    "# --- 1. HARDWARE SETUP ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ POWER UNLEASHED: Using Apple M4 (MPS) Acceleration\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"‚ö†Ô∏è WARNING: Running on CPU.\")\n",
    "\n",
    "# --- 2. FIND FILES ---\n",
    "print(\"\\nüîç Scanning for Test Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "\n",
    "if not input_a or not input_b:\n",
    "    input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "    print(\"‚ö†Ô∏è Using manual filenames (Auto-detection failed)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Track A File: {input_a}\")\n",
    "    print(f\"   ‚úÖ Track B File: {input_b}\")\n",
    "\n",
    "# --- 3. DOWNLOAD MODELS WITH PROGRESS BARS ---\n",
    "print(\"\\n‚¨áÔ∏è STARTING DOWNLOADS (This ensures you see progress)...\")\n",
    "\n",
    "# Model 1: DeBERTa v3 Large\n",
    "print(\"   1. Downloading DeBERTa-v3-Large (~800MB)...\")\n",
    "model_a_id = 'cross-encoder/nli-deberta-v3-large'\n",
    "snapshot_download(repo_id=model_a_id) # This triggers the bar\n",
    "\n",
    "# Model 2: GTE Large\n",
    "print(\"   2. Downloading GTE-Large-v1.5 (~1.5GB)...\")\n",
    "model_b_id = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "snapshot_download(repo_id=model_b_id) # This triggers the bar\n",
    "\n",
    "print(\"\\n‚úÖ Downloads Complete. Loading into Memory...\")\n",
    "model_a = CrossEncoder(model_a_id, device=device)\n",
    "model_b = SentenceTransformer(model_b_id, trust_remote_code=True, device=device)\n",
    "\n",
    "# --- 4. EXECUTE TRACK A ---\n",
    "print(f\"\\nüß† SCORING TRACK A ({input_a})...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "\n",
    "# Column detection\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "# Create pairs\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# INFERENCE (Progress bar included)\n",
    "scores_a = model_a.predict(pairs_a, batch_size=4, show_progress_bar=True)\n",
    "scores_b = model_a.predict(pairs_b, batch_size=4, show_progress_bar=True)\n",
    "preds_a = scores_a > scores_b\n",
    "\n",
    "# --- 5. EXECUTE TRACK B ---\n",
    "print(f\"\\nüß† EMBEDDING TRACK B ({input_b})...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    # INFERENCE (Progress bar included)\n",
    "    embeddings = model_b.encode(\n",
    "        df_b[text_col].tolist(), \n",
    "        batch_size=4, \n",
    "        show_progress_bar=True, \n",
    "        device=device,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. ZIP IT UP ---\n",
    "print(\"\\nüì¶ Zipping Final Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_SOTA_FINAL.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ READY! Upload '{zip_name}' to CodaBench Testing Phase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe901261-daf4-4839-9477-8eb75c1a5b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple M4 (MPS) - Ready for SOTA Models\n",
      "üîç Scanning for Test Data...\n",
      "   Track A File: test_track_a.jsonl\n",
      "   Track B File: test_track_b.jsonl\n",
      "\n",
      "üìÇ Loading Models from your local download...\n",
      "   ‚úÖ Found local DeBERTa model! Loading...\n",
      "   ‚úÖ Found local GTE model! Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Scoring Track A (DeBERTa)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:17<00:00,  3.94s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:59<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Embedding Track B (GTE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [01:24<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Zipping Submission...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/track_a.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m preds_a:\n\u001b[0;32m---> 97\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_a_is_closer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m}, f)\n\u001b[1;32m     98\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/track_b.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# --- 1. HARDWARE SETUP ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - Ready for SOTA Models\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"‚ö†Ô∏è Using CPU (Slow)\")\n",
    "\n",
    "# --- 2. FIND TEST FILES ---\n",
    "print(\"üîç Scanning for Test Data...\")\n",
    "# We look for files with the correct line counts (400 and 849)\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "\n",
    "# Fallback if auto-detection fails\n",
    "if not input_a or not input_b: \n",
    "    input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "\n",
    "print(f\"   Track A File: {input_a}\\n   Track B File: {input_b}\")\n",
    "\n",
    "# --- 3. LOAD MODELS FROM LOCAL FOLDERS ---\n",
    "print(\"\\nüìÇ Loading Models from your local download...\")\n",
    "\n",
    "# These match the folder names from the terminal command\n",
    "path_a = './nli-deberta-v3-large'\n",
    "path_b = './gte-large-en-v1.5'\n",
    "\n",
    "# Load Model A (DeBERTa)\n",
    "if os.path.exists(path_a):\n",
    "    print(\"   ‚úÖ Found local DeBERTa model! Loading...\")\n",
    "    model_a = CrossEncoder(path_a, device=device)\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: Could not find folder '{path_a}'. Check where you ran the terminal command.\")\n",
    "\n",
    "# Load Model B (GTE)\n",
    "if os.path.exists(path_b):\n",
    "    print(\"   ‚úÖ Found local GTE model! Loading...\")\n",
    "    model_b = SentenceTransformer(path_b, trust_remote_code=True, device=device)\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: Could not find folder '{path_b}'.\")\n",
    "\n",
    "# --- 4. RUN TRACK A (SCORING) ---\n",
    "print(f\"\\nüß† Scoring Track A (DeBERTa)...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "\n",
    "# Column detection\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# Batch size 8 is safe for M4 with these local models\n",
    "scores_a = model_a.predict(pairs_a, batch_size=8, show_progress_bar=True)\n",
    "scores_b = model_a.predict(pairs_b, batch_size=8, show_progress_bar=True)\n",
    "preds_a = scores_a > scores_b\n",
    "\n",
    "# --- 5. RUN TRACK B (EMBEDDING) ---\n",
    "print(f\"\\nüß† Embedding Track B (GTE)...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    embeddings = model_b.encode(\n",
    "        df_b[text_col].tolist(), \n",
    "        batch_size=4, # GTE Large is big, keep batch small\n",
    "        show_progress_bar=True, \n",
    "        device=device,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. ZIP AND FINISH ---\n",
    "print(\"\\nüì¶ Zipping Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_SOTA_LOCAL.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ READY! Upload '{zip_name}' to CodaBench Testing Phase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb570b74-db6e-4e69-a903-bf5c90fa93ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Applying Fix for NLI Model Output...\n",
      "   - Detected multi-column scores (400, 3). Extracting 'Entailment' (Index 1)...\n",
      "   - Re-calculated 400 predictions.\n",
      "üì¶ Zipping Final Submission...\n",
      "\n",
      "üèÜ SUCCESS! Upload 'submission_SOTA_FIXED.zip' to CodaBench Testing Phase.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"üõ†Ô∏è Applying Fix for NLI Model Output...\")\n",
    "\n",
    "# 1. FIX THE SCORES\n",
    "# The model output shape is likely (400, 3). We want column 1 (Entailment).\n",
    "# We check the shape to be safe.\n",
    "if len(scores_a.shape) > 1 and scores_a.shape[1] >= 2:\n",
    "    print(f\"   - Detected multi-column scores {scores_a.shape}. Extracting 'Entailment' (Index 1)...\")\n",
    "    final_scores_a = scores_a[:, 1]\n",
    "    final_scores_b = scores_b[:, 1]\n",
    "else:\n",
    "    # Fallback if it was already 1D\n",
    "    final_scores_a = scores_a\n",
    "    final_scores_b = scores_b\n",
    "\n",
    "# 2. RE-CALCULATE PREDICTIONS\n",
    "# Now we compare single numbers, so we get a clean True/False list\n",
    "preds_a = final_scores_a > final_scores_b\n",
    "print(f\"   - Re-calculated {len(preds_a)} predictions.\")\n",
    "\n",
    "# 3. SAVE & ZIP (Standard Routine)\n",
    "print(\"üì¶ Zipping Final Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        # This will now work because 'val' is a simple Python boolean\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "# We re-save Track B just to be sure (it was already fine, but good to keep in sync)\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_SOTA_FIXED.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ SUCCESS! Upload '{zip_name}' to CodaBench Testing Phase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d551f3dd-ab10-4535-98bd-645c9407807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç FINAL VERIFICATION PROTOCOL INITIATED...\n",
      "\n",
      "Checking Track A (outputs/track_a.jsonl)...\n",
      "   ‚úÖ Key 'text_a_is_closer' found.\n",
      "   ‚úÖ Value type is BOOLEAN (False).\n",
      "   ‚úÖ Line count is exactly 400.\n",
      "\n",
      "Checking Track B (outputs/track_b.jsonl)...\n",
      "   ‚úÖ Key 'embedding' (singular) found.\n",
      "   ‚úÖ Value is a LIST of floats (Length: 1024).\n",
      "   ‚úÖ Line count is exactly 849.\n",
      "\n",
      "Checking Zip Archive (submission_SOTA_FIXED.zip)...\n",
      "   ‚úÖ Zip contains correct files: ['track_a.jsonl', 'track_b.jsonl']\n",
      "\n",
      "üöÄ VERIFICATION COMPLETE. If all ticks are Green, you are safe to upload.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"üîç FINAL VERIFICATION PROTOCOL INITIATED...\\n\")\n",
    "\n",
    "# 1. Verify Track A\n",
    "print(\"Checking Track A (outputs/track_a.jsonl)...\")\n",
    "try:\n",
    "    with open('outputs/track_a.jsonl', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        count_a = len(lines)\n",
    "        first_line = json.loads(lines[0])\n",
    "        \n",
    "        # Check 1: Key Name\n",
    "        if \"text_a_is_closer\" in first_line:\n",
    "            print(f\"   ‚úÖ Key 'text_a_is_closer' found.\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå CRITICAL: Wrong key in Track A. Found: {first_line.keys()}\")\n",
    "            \n",
    "        # Check 2: Value Type\n",
    "        val = first_line[\"text_a_is_closer\"]\n",
    "        if isinstance(val, bool):\n",
    "            print(f\"   ‚úÖ Value type is BOOLEAN ({val}).\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå CRITICAL: Wrong type. Expected bool, got {type(val)}.\")\n",
    "            \n",
    "        # Check 3: Count\n",
    "        if count_a == 400:\n",
    "            print(f\"   ‚úÖ Line count is exactly 400.\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è WARNING: Line count is {count_a} (Expected 400).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error reading Track A: {e}\")\n",
    "\n",
    "# 2. Verify Track B\n",
    "print(\"\\nChecking Track B (outputs/track_b.jsonl)...\")\n",
    "try:\n",
    "    with open('outputs/track_b.jsonl', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        count_b = len(lines)\n",
    "        first_line = json.loads(lines[0])\n",
    "        \n",
    "        # Check 1: Key Name\n",
    "        if \"embedding\" in first_line:\n",
    "            print(f\"   ‚úÖ Key 'embedding' (singular) found.\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå CRITICAL: Wrong key. Found: {first_line.keys()}\")\n",
    "            \n",
    "        # Check 2: Value Type & Shape\n",
    "        emb = first_line[\"embedding\"]\n",
    "        if isinstance(emb, list) and len(emb) > 10:\n",
    "            print(f\"   ‚úÖ Value is a LIST of floats (Length: {len(emb)}).\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå CRITICAL: Invalid embedding format.\")\n",
    "            \n",
    "        # Check 3: Count\n",
    "        if count_b == 849:\n",
    "            print(f\"   ‚úÖ Line count is exactly 849.\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è WARNING: Line count is {count_b} (Expected ~849).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error reading Track B: {e}\")\n",
    "\n",
    "# 3. Verify Zip File\n",
    "print(\"\\nChecking Zip Archive (submission_SOTA_FIXED.zip)...\")\n",
    "try:\n",
    "    with zipfile.ZipFile('submission_BGE.zip', 'r') as z:\n",
    "        files = z.namelist()\n",
    "        if 'track_a.jsonl' in files and 'track_b.jsonl' in files:\n",
    "            print(f\"   ‚úÖ Zip contains correct files: {files}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå CRITICAL: Zip is missing files. Found: {files}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error checking Zip: {e}\")\n",
    "\n",
    "print(\"\\nüöÄ VERIFICATION COMPLETE. If all ticks are Green, you are safe to upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9decb094-f4cd-449f-8113-ee7ca470c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple M4 (MPS) - BGE Edition\n",
      "üîç Finding Test Data...\n",
      "   Track A: test_track_a.jsonl\n",
      "   Track B: test_track_b.jsonl\n",
      "\n",
      "üìÇ Loading BGE Models...\n",
      "   ‚úÖ Loading Reranker (Track A)...\n",
      "   ‚úÖ Loading Embedder (Track B)...\n",
      "\n",
      "üß† Scoring Track A (BGE Reranker)...\n",
      "   - Calculating scores...\n",
      "\n",
      "üß† Embedding Track B (BGE Large)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [01:04<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Zipping BGE Submission...\n",
      "\n",
      "üèÜ READY! Upload 'submission_BGE.zip' to CodaBench.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# --- 1. HARDWARE ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"üöÄ Using Apple M4 (MPS) - BGE Edition\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# --- 2. FIND TEST FILES ---\n",
    "print(\"üîç Finding Test Data...\")\n",
    "jsonl_files = glob.glob(\"*.jsonl\") + glob.glob(\"data/*.jsonl\") + glob.glob(\"SemEval_Task4/*.jsonl\")\n",
    "input_a, input_b = None, None\n",
    "for f in jsonl_files:\n",
    "    try:\n",
    "        count = sum(1 for line in open(f))\n",
    "        if count == 400: input_a = f\n",
    "        elif count == 849: input_b = f\n",
    "    except: pass\n",
    "if not input_a or not input_b: input_a, input_b = 'test_track_a.jsonl', 'test_track_b.jsonl'\n",
    "print(f\"   Track A: {input_a}\\n   Track B: {input_b}\")\n",
    "\n",
    "# --- 3. LOAD LOCAL BGE MODELS ---\n",
    "print(\"\\nüìÇ Loading BGE Models...\")\n",
    "\n",
    "# Path A: Reranker\n",
    "path_a = './bge-reranker-large'\n",
    "if os.path.exists(path_a):\n",
    "    print(\"   ‚úÖ Loading Reranker (Track A)...\")\n",
    "    # Rerankers are loaded slightly differently than CrossEncoders\n",
    "    tokenizer_a = AutoTokenizer.from_pretrained(path_a)\n",
    "    model_a = AutoModelForSequenceClassification.from_pretrained(path_a).to(device)\n",
    "    model_a.eval()\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: '{path_a}' not found. Did you run the terminal command?\")\n",
    "\n",
    "# Path B: Embedding\n",
    "path_b = './bge-large-en-v1.5'\n",
    "if os.path.exists(path_b):\n",
    "    print(\"   ‚úÖ Loading Embedder (Track B)...\")\n",
    "    model_b = SentenceTransformer(path_b, device=device)\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: '{path_b}' not found.\")\n",
    "\n",
    "# --- 4. RUN TRACK A (RERANKING) ---\n",
    "print(f\"\\nüß† Scoring Track A (BGE Reranker)...\")\n",
    "df_a = pd.read_json(input_a, lines=True)\n",
    "\n",
    "anc_col = next((c for c in ['anchor_text', 'anchor'] if c in df_a.columns), 'anchor')\n",
    "a_col = next((c for c in ['text_a', 'a'] if c in df_a.columns), 'a')\n",
    "b_col = next((c for c in ['text_b', 'b'] if c in df_a.columns), 'b')\n",
    "\n",
    "# Reranker expects simple pairs: [Anchor, A] and [Anchor, B]\n",
    "pairs_a = df_a[[anc_col, a_col]].values.tolist()\n",
    "pairs_b = df_a[[anc_col, b_col]].values.tolist()\n",
    "\n",
    "# Helper function for Reranker Inference\n",
    "def predict_reranker(pairs, model, tokenizer, batch_size=8):\n",
    "    scores = []\n",
    "    # Process in chunks\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[i:i+batch_size]\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            # Get logits (score)\n",
    "            output = model(**inputs).logits.view(-1).float()\n",
    "            scores.extend(output.cpu().numpy())\n",
    "    return scores\n",
    "\n",
    "print(\"   - Calculating scores...\")\n",
    "scores_a = predict_reranker(pairs_a, model_a, tokenizer_a)\n",
    "scores_b = predict_reranker(pairs_b, model_a, tokenizer_a)\n",
    "\n",
    "# Logic: Higher score = Better match\n",
    "preds_a = [s_a > s_b for s_a, s_b in zip(scores_a, scores_b)]\n",
    "\n",
    "# --- 5. RUN TRACK B (EMBEDDING) ---\n",
    "print(f\"\\nüß† Embedding Track B (BGE Large)...\")\n",
    "df_b = pd.read_json(input_b, lines=True)\n",
    "text_col = next((c for c in ['text', 'story', 'anchor', 'anchor_text'] if c in df_b.columns), None)\n",
    "\n",
    "if text_col:\n",
    "    # BGE works best with a prompt for asymmetric tasks, but for symmetric story similarity\n",
    "    # we usually keep it raw. However, adding \"Represent this story:\" can sometimes help.\n",
    "    # Let's stick to raw for safety unless specified.\n",
    "    embeddings = model_b.encode(\n",
    "        df_b[text_col].tolist(), \n",
    "        batch_size=8, \n",
    "        show_progress_bar=True, \n",
    "        device=device,\n",
    "        normalize_embeddings=True # BGE requires normalized embeddings\n",
    "    )\n",
    "    embeddings_list = embeddings.tolist()\n",
    "else:\n",
    "    embeddings_list = []\n",
    "\n",
    "# --- 6. SAVE & ZIP ---\n",
    "print(\"\\nüì¶ Zipping BGE Submission...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "with open('outputs/track_a.jsonl', 'w') as f:\n",
    "    for val in preds_a:\n",
    "        json.dump({\"text_a_is_closer\": bool(val)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('outputs/track_b.jsonl', 'w') as f:\n",
    "    for emb in embeddings_list:\n",
    "        json.dump({\"embedding\": emb}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "zip_name = 'submission_BGE.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('outputs/track_a.jsonl', arcname='track_a.jsonl')\n",
    "    zipf.write('outputs/track_b.jsonl', arcname='track_b.jsonl')\n",
    "\n",
    "print(f\"\\nüèÜ READY! Upload '{zip_name}' to CodaBench.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5fe4e15-a7a2-4578-8643-f9de62366f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VERIFYING: submission_BGE.zip ...\n",
      "\n",
      "   ‚úÖ ZIP Structure: OK (Found both jsonl files)\n",
      "   ‚úÖ Track A Count: OK (400 items)\n",
      "   ‚úÖ Track A Format: OK (Key 'text_a_is_closer' is Boolean)\n",
      "   ‚úÖ Track B Count: OK (849 items)\n",
      "   ‚úÖ Track B Key: OK (Found singular 'embedding')\n",
      "   ‚úÖ Track B Dimensions: OK (1024 for BGE-Large)\n",
      "\n",
      "üöÄ STATUS: READY TO UPLOAD.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_filename = 'submission_BGE.zip'\n",
    "\n",
    "print(f\"üîç VERIFYING: {zip_filename} ...\\n\")\n",
    "\n",
    "if not os.path.exists(zip_filename):\n",
    "    print(f\"‚ùå CRITICAL ERROR: File '{zip_filename}' not found!\")\n",
    "else:\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as z:\n",
    "            files = z.namelist()\n",
    "            \n",
    "            # --- CHECK 1: FILE STRUCTURE ---\n",
    "            if 'track_a.jsonl' in files and 'track_b.jsonl' in files:\n",
    "                print(f\"   ‚úÖ ZIP Structure: OK (Found both jsonl files)\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå ZIP ERROR: Missing files. Found: {files}\")\n",
    "\n",
    "            # --- CHECK 2: TRACK A CONTENT ---\n",
    "            with z.open('track_a.jsonl') as f:\n",
    "                lines = f.readlines()\n",
    "                count = len(lines)\n",
    "                first = json.loads(lines[0])\n",
    "                \n",
    "                # Check Count\n",
    "                if count == 400:\n",
    "                    print(f\"   ‚úÖ Track A Count: OK (400 items)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è Track A Count: WARNING ({count} items - Expected 400)\")\n",
    "\n",
    "                # Check Key & Type\n",
    "                if \"text_a_is_closer\" in first and isinstance(first[\"text_a_is_closer\"], bool):\n",
    "                    print(f\"   ‚úÖ Track A Format: OK (Key 'text_a_is_closer' is Boolean)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Track A ERROR: Invalid JSON format: {first}\")\n",
    "\n",
    "            # --- CHECK 3: TRACK B CONTENT ---\n",
    "            with z.open('track_b.jsonl') as f:\n",
    "                lines = f.readlines()\n",
    "                count = len(lines)\n",
    "                first = json.loads(lines[0])\n",
    "                \n",
    "                # Check Count\n",
    "                if count == 849:\n",
    "                    print(f\"   ‚úÖ Track B Count: OK (849 items)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è Track B Count: WARNING ({count} items - Expected 849)\")\n",
    "\n",
    "                # Check Key (The most common error)\n",
    "                if \"embedding\" in first:\n",
    "                    print(f\"   ‚úÖ Track B Key: OK (Found singular 'embedding')\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Track B ERROR: Key mismatch! Found: {list(first.keys())} (Expected 'embedding')\")\n",
    "\n",
    "                # Check Vector Size (BGE-Large should be 1024)\n",
    "                vec_len = len(first[\"embedding\"])\n",
    "                if vec_len == 1024:\n",
    "                    print(f\"   ‚úÖ Track B Dimensions: OK (1024 for BGE-Large)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ÑπÔ∏è Track B Dimensions: {vec_len} (Just FYI)\")\n",
    "\n",
    "        print(\"\\nüöÄ STATUS: READY TO UPLOAD.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå SCRIPT CRASHED: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
